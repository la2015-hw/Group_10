{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/la2015-hw/Group_10/blob/main/extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Please Read Before Running following Code Cells**\n",
        "\n",
        "\n",
        "Get Kaggle API key\n",
        "1. Navigate to Kaggle\n",
        "2. Naviagte to the Settings Page (Ensure you have a Kaggle account)\n",
        "3. Scroll to API and click \"Create New Token\"\n",
        "4. A JSON file downloads on you local machine\n",
        "\n",
        "Set Up Kaggle API\n",
        "1. Navigate to Google colab\n",
        "2. Click on files and click upload to session storage\n",
        "3. Once uploaded run the following commands in terminal\n",
        "\n",
        "```\n",
        "mkdir -p ~/.kaggle\n",
        "cp -f /content/kaggle.json ~/.kaggle/kaggle.json\n",
        "chmod 600 ~/.kaggle/kaggle.json\n",
        "```\n",
        "\n",
        "**If that does not work try this**\n",
        "\n",
        "```\n",
        "mkdir -p ~/.kaggle\n",
        "cp -f kaggle.json ~/.kaggle/kaggle.json\n",
        "chmod 600 ~/.kaggle/kaggle.json\n",
        "```\n",
        "4. API key has been set up and now you can read and write.\n",
        "\n",
        "**Reminder - Setup is necessary after each Session**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A40B4-sFryui"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wggR1uw4c7S3"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"mustansireranpurwala/sdss-image-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "-TNC7ctG_PRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Image data Manipulation**"
      ],
      "metadata": {
        "id": "fa1SvgHQjgsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/home/mte/sdss_w_specz_train.h5'\n",
        "gz2_path = 'zoo2MainSpecz.csv'\n",
        "output_path = 'processed_sdss_only.h5'\n",
        "chunk_size = 25000\n",
        "\n",
        "# --- HDF5 helper functions ---\n",
        "def create_ds(f, name, dtype, str_len=None):\n",
        "    if dtype == 'S':\n",
        "        return f.create_dataset(name, shape=(0,), maxshape=(None,), dtype=f'S{str_len}', compression='gzip')\n",
        "    else:\n",
        "        return f.create_dataset(name, shape=(0,), maxshape=(None,), dtype=dtype, compression='gzip')\n",
        "\n",
        "def append_chunk(ds, data):\n",
        "    old_len = ds.shape[0]\n",
        "    new_len = old_len + data.shape[0]\n",
        "    ds.resize((new_len, *ds.shape[1:]))\n",
        "    ds[old_len:] = data\n",
        "\n",
        "\n",
        "# --- Magnitude & color functions ---\n",
        "def flux_to_magnitude(flux, zero_point=22.5):\n",
        "    flux = np.maximum(flux, 1e-10)\n",
        "    return -2.5 * np.log10(flux) + zero_point\n",
        "\n",
        "def extract_magnitudes_from_image(image_5band):\n",
        "    mags = []\n",
        "    for band_idx in range(5):\n",
        "        band_image = image_5band[:, :, band_idx]\n",
        "        total_flux = np.sum(band_image)\n",
        "        mags.append(flux_to_magnitude(total_flux))\n",
        "    return mags\n",
        "\n",
        "def calculate_colors(u, g, r, i, z, e_bv):\n",
        "    A_u = 5.155 * e_bv\n",
        "    A_g = 3.793 * e_bv\n",
        "    A_r = 2.751 * e_bv\n",
        "    A_i = 2.086 * e_bv\n",
        "    A_z = 1.479 * e_bv\n",
        "    u_corr = u - A_u\n",
        "    g_corr = g - A_g\n",
        "    r_corr = r - A_r\n",
        "    i_corr = i - A_i\n",
        "    z_corr = z - A_z\n",
        "    return {\n",
        "        'u_mag': u_corr, 'g_mag': g_corr, 'r_mag': r_corr,\n",
        "        'i_mag': i_corr, 'z_mag': z_corr,\n",
        "        'u_minus_g': u_corr - g_corr, 'g_minus_r': g_corr - r_corr,\n",
        "        'r_minus_i': r_corr - i_corr, 'i_minus_z': i_corr - z_corr\n",
        "    }\n",
        "\n",
        "def classify_galaxy_death(u_minus_g, g_minus_r):\n",
        "    labels = []\n",
        "    for ug, gr in zip(u_minus_g, g_minus_r):\n",
        "        if ug > 1.5 and gr > 0.8:\n",
        "            labels.append('DEAD')\n",
        "        elif ug < 1.0 and gr < 0.6:\n",
        "            labels.append('ALIVE')\n",
        "        else:\n",
        "            labels.append('TRANSITIONAL')\n",
        "    return labels\n",
        "\n",
        "# --- Load Galaxy Zoo 2 dataset ---\n",
        "gz2 = pd.read_csv(gz2_path)\n",
        "gz2 = gz2[pd.to_numeric(gz2['specobjid'], errors='coerce').notna()]\n",
        "gz2['specobjid'] = gz2['specobjid'].astype(np.int64)\n",
        "gz2_specids = set(gz2['specobjid'].values)\n",
        "\n",
        "# --- Process SDSS in chunks and merge on specObjID ---\n",
        "with h5py.File(file_path, 'r') as f_in, h5py.File(output_path, 'w') as f_out:\n",
        "    N = f_in['images'].shape[0]\n",
        "    print(f\"Processing {N} galaxies in chunks of {chunk_size}...\")\n",
        "\n",
        "    # Create output datasets\n",
        "    images_ds = f_out.create_dataset('images', shape=(0, 107, 107, 5), maxshape=(None, 107, 107, 5), dtype='float32', compression='gzip')\n",
        "    objid_ds = create_ds(f_out, 'ObjID', 'S', 20)\n",
        "    ra_ds = create_ds(f_out, 'ra', 'f8')\n",
        "    dec_ds = create_ds(f_out, 'dec', 'f8')\n",
        "    e_bv_ds = create_ds(f_out, 'e_bv', 'f4')\n",
        "    spec_ds = create_ds(f_out, 'specObjID', 'i8')\n",
        "    red_ds = create_ds(f_out, 'specz_redshift', 'f4')\n",
        "    red_err_ds = create_ds(f_out, 'specz_redshift_err', 'f4')\n",
        "    u_ds = create_ds(f_out, 'u_mag', 'f4')\n",
        "    g_ds = create_ds(f_out, 'g_mag', 'f4')\n",
        "    r_ds = create_ds(f_out, 'r_mag', 'f4')\n",
        "    i_ds = create_ds(f_out, 'i_mag', 'f4')\n",
        "    z_ds = create_ds(f_out, 'z_mag', 'f4')\n",
        "    ug_ds = create_ds(f_out, 'u_minus_g', 'f4')\n",
        "    gr_ds = create_ds(f_out, 'g_minus_r', 'f4')\n",
        "    ri_ds = create_ds(f_out, 'r_minus_i', 'f4')\n",
        "    iz_ds = create_ds(f_out, 'i_minus_z', 'f4')\n",
        "    death_ds = create_ds(f_out, 'death_status', 'S', 15)\n",
        "\n",
        "    for start in range(0, N, chunk_size):\n",
        "        end = min(start + chunk_size, N)\n",
        "        print(f\"Chunk {start}-{end-1}\")\n",
        "\n",
        "        # Load chunk\n",
        "        img_chunk = f_in['images'][start:end]\n",
        "        if img_chunk.shape[1] == 5 and img_chunk.shape[2] == 107:\n",
        "            img_chunk = np.transpose(img_chunk, (0, 2, 3, 1))\n",
        "\n",
        "        spec_chunk = np.array([int(x.decode('utf-8')) for x in f_in['specObjID'][start:end]])\n",
        "\n",
        "        # Filter only galaxies present in Galaxy Zoo 2\n",
        "        mask = np.array([s in gz2_specids for s in spec_chunk])\n",
        "        if np.sum(mask) == 0:\n",
        "            continue\n",
        "\n",
        "        img_chunk = img_chunk[mask]\n",
        "        obj_chunk = f_in['ObjID'][start:end][mask]\n",
        "        ra_chunk = f_in['ra'][start:end][mask]\n",
        "        dec_chunk = f_in['dec'][start:end][mask]\n",
        "        e_bv_chunk = f_in['e_bv'][start:end][mask]\n",
        "        spec_chunk = spec_chunk[mask]\n",
        "        red_chunk = f_in['specz_redshift'][start:end][mask]\n",
        "        red_err_chunk = f_in['specz_redshift_err'][start:end][mask]\n",
        "\n",
        "        # Magnitudes\n",
        "        chunk_mags = np.array([extract_magnitudes_from_image(img) for img in img_chunk])\n",
        "        u_c, g_c, r_c, i_c, z_c = chunk_mags.T\n",
        "\n",
        "        # Colors & death labels\n",
        "        colors = calculate_colors(u_c, g_c, r_c, i_c, z_c, e_bv_chunk)\n",
        "        deaths = classify_galaxy_death(colors['u_minus_g'], colors['g_minus_r'])\n",
        "\n",
        "        # Append to datasets\n",
        "        append_chunk(images_ds, img_chunk)\n",
        "        append_chunk(objid_ds, np.array([x.decode('utf-8').encode('utf-8') for x in obj_chunk]))\n",
        "        append_chunk(ra_ds, ra_chunk.astype(float))\n",
        "        append_chunk(dec_ds, dec_chunk.astype(float))\n",
        "        append_chunk(e_bv_ds, e_bv_chunk)\n",
        "        append_chunk(spec_ds, spec_chunk)\n",
        "        append_chunk(red_ds, red_chunk)\n",
        "        append_chunk(red_err_ds, red_err_chunk)\n",
        "        append_chunk(u_ds, colors['u_mag'])\n",
        "        append_chunk(g_ds, colors['g_mag'])\n",
        "        append_chunk(r_ds, colors['r_mag'])\n",
        "        append_chunk(i_ds, colors['i_mag'])\n",
        "        append_chunk(z_ds, colors['z_mag'])\n",
        "        append_chunk(ug_ds, colors['u_minus_g'])\n",
        "        append_chunk(gr_ds, colors['g_minus_r'])\n",
        "        append_chunk(ri_ds, colors['r_minus_i'])\n",
        "        append_chunk(iz_ds, colors['i_minus_z'])\n",
        "        append_chunk(death_ds, np.array([d.encode('utf-8') for d in deaths]))\n",
        "\n",
        "print(f\"Saved matched SDSS images and metadata to {output_path}\")"
      ],
      "metadata": {
        "id": "FdjWPCoHjaMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**To push data back to kaggle**\n",
        "\n",
        "1. Run the following code to create a folder to be pushed back which contains a metadata file"
      ],
      "metadata": {
        "id": "tyTnQ75xHcIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "dataset_folder = \"/content/processed_dataset\"\n",
        "\n",
        "os.makedirs(dataset_folder, exist_ok=True)\n",
        "\n",
        "metadata = {\n",
        "    \"title\": \"SDSS Image Dataset - Processed\",\n",
        "    \"id\": \"mustansireranpurwala/sdss-image-dataset\",\n",
        "    \"licenses\": [\n",
        "        {\"name\": \"CC0-1.0\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open(f\"{dataset_folder}/dataset-metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "\n",
        "print(\"dataset-metadata.json created in\", dataset_folder)"
      ],
      "metadata": {
        "id": "7C0RlMV0H5yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Open Terminal and run the following commands\n",
        "\n",
        "```\n",
        "cp -r /root/.cache/kagglehub/datasets/mustansireranpurwala/sdss-image-dataset/versions/1/sdss_w_specz_valid.h5 /content/processed_dataset/\n",
        "```\n",
        "This command moves the dataset from the root to the processed_data folder\n",
        "```\n",
        "kaggle datasets version -p /content/my_dataset_to_push -m \"Test push from Colab\"\n",
        "```\n",
        "This command initiates the push to kaggle\n",
        "\n",
        "3. The processed dataset has sucessfully been pushed"
      ],
      "metadata": {
        "id": "CsETUay6IFJk"
      }
    }
  ]
}